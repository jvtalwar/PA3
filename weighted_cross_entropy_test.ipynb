{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import utils\n",
    "from basic_fcn import *\n",
    "from dataloader import *\n",
    "from utils import *\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 2\n",
    "train_dataset = CityScapesDataset(csv_file='train.csv')\n",
    "val_dataset = CityScapesDataset(csv_file='val.csv')\n",
    "test_dataset = CityScapesDataset(csv_file='test.csv')\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset,\n",
    "                          batch_size=batch,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        torch.nn.init.xavier_uniform_(m.bias.data.unsqueeze(0)) #add unsqueeze here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy(x, y, weights):\n",
    "    probability = -1.0 * F.log_softmax(x, dim = 1)\n",
    "    \n",
    "    loss = probability.gather(1, y.unsqueeze(1))\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0994, 0.1993, 0.0520, 0.1805, 0.3327, 0.3999, 0.4218, 0.3714, 0.3637,\n",
      "        0.6260, 0.6885, 0.9683, 0.5666, 0.5976, 0.7129, 0.5414, 0.6813, 0.7918,\n",
      "        0.2177, 0.4594, 0.8826, 0.9867, 0.5193, 0.4215, 0.6516, 0.0564, 0.5730,\n",
      "        0.6652, 0.2092, 0.8599, 0.9266, 0.8547, 0.3839, 0.2919],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(34).cuda()\n",
    "print(x)\n",
    "criterion = nn.CrossEntropyLoss(weight=x)\n",
    "criterion_unweighted = nn.CrossEntropyLoss()\n",
    "x = x.view(1, 34, 1, 1).cuda()\n",
    "fcn_test = FCN(n_class=n_class)\n",
    "fcn_test.apply(init_weights)\n",
    "fcn_test = fcn_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My weighted loss function outputs: 2.1941280364990234\n",
      "My unweighted loss function outputs: 3.605480670928955\n",
      "PyTorches weighted loss function outputs: 3.5787203311920166\n",
      "PyTorches unweighted loss function outputs: 3.605480432510376\n"
     ]
    }
   ],
   "source": [
    "# Test weighted cross-entropy\n",
    "fcn_test.eval()\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    inputs = sample_batched[0].cuda()\n",
    "    one_hot_labels = sample_batched[1].cuda()\n",
    "    labels = sample_batched[2].cuda()\n",
    "    outputs = fcn_test(inputs)\n",
    "    del inputs\n",
    "    \n",
    "    probability = -1.0 * F.log_softmax(outputs, dim = 1)\n",
    "    weighted_probability = probability*x\n",
    "    loss_weighted_self = weighted_probability.gather(1, labels.unsqueeze(1))\n",
    "    loss_weighted_self = loss_weighted_self.mean()\n",
    "    print(\"My weighted loss function outputs:\", loss_weighted_self.item())\n",
    "    \n",
    "    loss_unweighted_self = probability.gather(1, labels.unsqueeze(1))\n",
    "    loss_unweighted_self = loss_unweighted_self.mean()\n",
    "    print(\"My unweighted loss function outputs:\", loss_unweighted_self.item())\n",
    "    \n",
    "    loss_weighted = criterion(outputs, labels)\n",
    "    print(\"PyTorches weighted loss function outputs:\", loss_weighted.item())\n",
    "    \n",
    "    loss_unweighted = criterion_unweighted(outputs, labels)\n",
    "    print(\"PyTorches unweighted loss function outputs:\", loss_unweighted.item())\n",
    "      \n",
    "    del one_hot_labels, labels, outputs, probability, weighted_probability, loss_unweighted, loss_unweighted_self, loss_weighted, loss_weighted_self\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory allocation: 22948864\n"
     ]
    }
   ],
   "source": [
    "print(\"Current memory allocation:\", torch.cuda.memory_allocated())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
